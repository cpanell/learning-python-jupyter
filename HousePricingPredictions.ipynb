{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('AmesHousing.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57088.25161263909"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating functions as a pipeline for predicting the data and returning the RMSE\n",
    "\n",
    "## Function for transforming the features based on criteria (initially none)\n",
    "def transform_features(df):\n",
    "    train = df\n",
    "    return train\n",
    "\n",
    "## Function for selecting features for predicting based on criteria (initially none)\n",
    "def select_features(df):\n",
    "    return df[['Gr Liv Area', 'SalePrice']]\n",
    "\n",
    "## Function for splitting a dataframe into train and test and returning the RMSE of the predictions\n",
    "def train_and_test(df):\n",
    "    train = df[:1460]\n",
    "    test = df[1460:]\n",
    "    \n",
    "    ## Filter out only the numeric columns\n",
    "    n_train = train.select_dtypes(include=['integer', 'float'])\n",
    "    n_test = test.select_dtypes(include=['integer', 'float'])\n",
    "    \n",
    "    ## Create a list of all columns without 'SalePrice'\n",
    "    features = n_train.columns.drop('SalePrice')\n",
    "    \n",
    "    ## Use linear regression to find the root mean squared error\n",
    "    lr = linear_model.LinearRegression()\n",
    "    lr.fit(train[features], train['SalePrice'])\n",
    "    predictions = lr.predict(test[features])\n",
    "    mse = mean_squared_error(test['SalePrice'], predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "## Test the functions\n",
    "transform_data = transform_features(data)\n",
    "filtered_data = select_features(transform_data)\n",
    "rmse = train_and_test(filtered_data)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sum of null values for each column\n",
    "null_values = data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop columns with less than 5% missing values\n",
    "over_5 = null_values[(null_values > len(data) * 0.05)].sort_values()\n",
    "data = data.drop(over_5.index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BsmtFin SF 1       1\n",
       "BsmtFin SF 2       1\n",
       "Bsmt Unf SF        1\n",
       "Total Bsmt SF      1\n",
       "Garage Cars        1\n",
       "Garage Area        1\n",
       "Bsmt Full Bath     2\n",
       "Bsmt Half Bath     2\n",
       "Mas Vnr Area      23\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Replace numeric columns with missing values with the mode for that column\n",
    "numeric_missing = data.select_dtypes(include=['float', 'int']).isnull().sum()\n",
    "fix_columns = numeric_missing[numeric_missing > 0].sort_values()\n",
    "fix_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BsmtFin SF 1': 0.0,\n",
       " 'BsmtFin SF 2': 0.0,\n",
       " 'Bsmt Unf SF': 0.0,\n",
       " 'Total Bsmt SF': 0.0,\n",
       " 'Garage Cars': 2.0,\n",
       " 'Garage Area': 0.0,\n",
       " 'Bsmt Full Bath': 0.0,\n",
       " 'Bsmt Half Bath': 0.0,\n",
       " 'Mas Vnr Area': 0.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a dictionary with the mode for each fixable numeric column\n",
    "replace_dict = data[fix_columns.index].mode().to_dict(orient='records')[0]\n",
    "replace_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the dictionary to fill in the values where missing\n",
    "data = data.fillna(replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    64\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find text columns with null values\n",
    "text_missing = data.select_dtypes(include='object').isnull().sum()\n",
    "\n",
    "## Remove columns with null values\n",
    "drop_text = text_missing[text_missing > 0]\n",
    "data = data.drop(drop_text.index, axis=1)\n",
    "\n",
    "## Verify that there are no more null values\n",
    "data.isnull().sum().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create new columns to improve the data\n",
    "\n",
    "## Create a column from Year Sold and Year Built\n",
    "data['Years Before Sale'] = data['Yr Sold'] - data['Year Built']\n",
    "\n",
    "## Create a column from Year Sold and Year Remodelled \n",
    "data['Years Since Remod'] = data['Yr Sold'] - data['Year Remod/Add']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2180   -1\n",
       "Name: Years Before Sale, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find rows with negative values for Years Before Sale\n",
    "data['Years Before Sale'][data['Years Before Sale'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1702   -1\n",
       "2180   -2\n",
       "2181   -1\n",
       "Name: Years Since Remod, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find rows with negative values for Years Since Remod\n",
    "data['Years Since Remod'][data['Years Since Remod'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop rows with negative values for the new columns\n",
    "data = data.drop([1702, 2180, 2181], axis=0)\n",
    "\n",
    "## Drop columns no longer necessary columns\n",
    "data = data.drop(['Year Built', 'Year Remod/Add'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop columns that are not useful for ML\n",
    "data = data.drop(['PID', 'Order'], axis=1)\n",
    "\n",
    "## Drop columns that leak data about the sale\n",
    "data = data.drop(['Mo Sold', 'Sale Condition', 'Sale Type', 'Yr Sold'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55275.367312413066"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Update the transform_features function with the findings above\n",
    "\n",
    "## Function for transforming the features based on criteria (initially none)\n",
    "def transform_features(df):\n",
    "    ## Sum of null values for each column\n",
    "    null_values = df.isnull().sum()\n",
    "    \n",
    "    ## Drop columns with less than 5% missing values\n",
    "    over_5 = null_values[(null_values > len(df) * 0.05)].sort_values()\n",
    "    df = df.drop(over_5.index, axis=1)\n",
    "    \n",
    "    ## Replace numeric columns with missing values with the mode for that column\n",
    "    numeric_missing = df.select_dtypes(include=['float', 'int']).isnull().sum()\n",
    "    fix_columns = numeric_missing[numeric_missing > 0].sort_values()\n",
    "    \n",
    "    ## Create a dictionary with the mode for each fixable numeric column\n",
    "    replace_dict = df[fix_columns.index].mode().to_dict(orient='records')[0]\n",
    "    \n",
    "    ## Use the dictionary to fill in the values where missing\n",
    "    df = df.fillna(replace_dict)\n",
    "    \n",
    "    ## Find text columns with null values\n",
    "    text_missing = df.select_dtypes(include='object').isnull().sum()\n",
    "    \n",
    "    ## Remove columns with null values\n",
    "    drop_text = text_missing[text_missing > 0]\n",
    "    df = df.drop(drop_text.index, axis=1)\n",
    "    \n",
    "    ## Create a column from Year Sold and Year Built\n",
    "    df['Years Before Sale'] = df['Yr Sold'] - df['Year Built']\n",
    "    \n",
    "    ## Create a column from Year Sold and Year Remodelled \n",
    "    df['Years Since Remod'] = df['Yr Sold'] - df['Year Remod/Add']\n",
    "    \n",
    "    ## Drop rows with negative values for the new columns\n",
    "    df = df.drop([1702, 2180, 2181], axis=0)\n",
    "    \n",
    "    ## Drop columns no longer necessary columns\n",
    "    df = df.drop(['Year Built', 'Year Remod/Add'], axis=1)\n",
    "    \n",
    "    ## Drop columns that are not useful for ML\n",
    "    df = df.drop(['PID', 'Order'], axis=1)\n",
    "    \n",
    "    ## Drop columns that leak data about the sale\n",
    "    df = df.drop(['Mo Sold', 'Sale Condition', 'Sale Type', 'Yr Sold'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "## Function for selecting features for predicting based on criteria (initially none)\n",
    "def select_features(df):\n",
    "    return df[['Gr Liv Area', 'SalePrice']]\n",
    "\n",
    "## Function for splitting a dataframe into train and test and returning the RMSE of the predictions\n",
    "def train_and_test(df):\n",
    "    train = df[:1460]\n",
    "    test = df[1460:]\n",
    "    \n",
    "    ## Filter out only the numeric columns\n",
    "    n_train = train.select_dtypes(include=['integer', 'float'])\n",
    "    n_test = test.select_dtypes(include=['integer', 'float'])\n",
    "    \n",
    "    ## Create a list of all columns without 'SalePrice'\n",
    "    features = n_train.columns.drop('SalePrice')\n",
    "    \n",
    "    ## Use linear regression to find the root mean squared error\n",
    "    lr = linear_model.LinearRegression()\n",
    "    lr.fit(train[features], train['SalePrice'])\n",
    "    predictions = lr.predict(test[features])\n",
    "    mse = mean_squared_error(test['SalePrice'], predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "## Test the functions\n",
    "df = pd.read_csv('AmesHousing.tsv', delimiter='\\t')\n",
    "transform_df = transform_features(df)\n",
    "filtered_df = select_features(transform_df)\n",
    "rmse = train_and_test(filtered_df)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BsmtFin SF 2         0.006127\n",
       "Misc Val             0.019273\n",
       "3Ssn Porch           0.032268\n",
       "Bsmt Half Bath       0.035875\n",
       "Low Qual Fin SF      0.037629\n",
       "Pool Area            0.068438\n",
       "MS SubClass          0.085128\n",
       "Overall Cond         0.101540\n",
       "Screen Porch         0.112280\n",
       "Kitchen AbvGr        0.119760\n",
       "Enclosed Porch       0.128685\n",
       "Bedroom AbvGr        0.143916\n",
       "Bsmt Unf SF          0.182751\n",
       "Lot Area             0.267520\n",
       "2nd Flr SF           0.269601\n",
       "Bsmt Full Bath       0.276258\n",
       "Half Bath            0.284871\n",
       "Open Porch SF        0.316262\n",
       "Wood Deck SF         0.328183\n",
       "BsmtFin SF 1         0.439284\n",
       "Fireplaces           0.474831\n",
       "TotRms AbvGrd        0.498574\n",
       "Mas Vnr Area         0.506983\n",
       "Years Since Remod    0.534985\n",
       "Full Bath            0.546118\n",
       "Years Before Sale    0.558979\n",
       "1st Flr SF           0.635185\n",
       "Garage Area          0.641425\n",
       "Total Bsmt SF        0.644012\n",
       "Garage Cars          0.648361\n",
       "Gr Liv Area          0.717596\n",
       "Overall Qual         0.801206\n",
       "SalePrice            1.000000\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find columns with a high correlation with SalePrice\n",
    "\n",
    "## Include only numeric columns\n",
    "numeric_df = transform_df.select_dtypes(include=['float', 'int'])\n",
    "\n",
    "## List columns with their correlation to SalePrice\n",
    "abs_corr = numeric_df.corr()['SalePrice'].abs().sort_values()\n",
    "\n",
    "abs_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use a correlation cutoff value to only include columns that have a higher correlation coefficient\n",
    "cor_cutoff = 0.4\n",
    "\n",
    "##Apply cor_cutoff to drop rows\n",
    "transform_df = transform_df.drop(abs_corr[abs_corr < cor_cutoff].index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Street           2\n",
       "Central Air      2\n",
       "Land Contour     4\n",
       "Lot Config       5\n",
       "Bldg Type        5\n",
       "Roof Style       6\n",
       "Foundation       6\n",
       "Heating          6\n",
       "MS Zoning        7\n",
       "Condition 2      8\n",
       "House Style      8\n",
       "Roof Matl        8\n",
       "Condition 1      9\n",
       "Exterior 1st    16\n",
       "Exterior 2nd    17\n",
       "Neighborhood    28\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## List of categorical columns from the documentation\n",
    "cat_columns = [\"PID\", \n",
    "               \"MS SubClass\", \n",
    "               \"MS Zoning\", \n",
    "               \"Street\", \n",
    "               \"Alley\", \n",
    "               \"Land Contour\", \n",
    "               \"Lot Config\", \n",
    "               \"Neighborhood\", \n",
    "               \"Condition 1\", \n",
    "               \"Condition 2\", \n",
    "               \"Bldg Type\", \n",
    "               \"House Style\", \n",
    "               \"Roof Style\", \n",
    "               \"Roof Matl\", \n",
    "               \"Exterior 1st\", \n",
    "               \"Exterior 2nd\", \n",
    "               \"Mas Vnr Type\", \n",
    "               \"Foundation\", \n",
    "               \"Heating\", \n",
    "               \"Central Air\", \n",
    "               \"Garage Type\", \n",
    "               \"Misc Feature\", \n",
    "               \"Sale Type\", \n",
    "               \"Sale Condition\"\n",
    "              ]\n",
    "\n",
    "## Limit the list to only those columns in our dataframe\n",
    "current_cat_columns = []\n",
    "\n",
    "for col in cat_columns:\n",
    "    if col in transform_df.columns:\n",
    "        current_cat_columns.append(col)\n",
    "\n",
    "## Find the unique values in each column\n",
    "uniq_counts = transform_df[current_cat_columns].apply(lambda col: len(col.value_counts())).sort_values()\n",
    "\n",
    "uniq_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will pick 10 as the default cutoff\n",
    "uniq_cutoff = 10\n",
    "\n",
    "## Apply the cutoff to the dataframe\n",
    "drop_uniq_cols = uniq_counts[uniq_counts > uniq_cutoff].index\n",
    "transform_df = transform_df.drop(drop_uniq_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a list of text columns\n",
    "text_cols = transform_df.select_dtypes(include=['object'])\n",
    "\n",
    "## Convert text columns to object columns\n",
    "for col in text_cols:\n",
    "    transform_df[col] = transform_df[col].astype('category')\n",
    "    \n",
    "## Create dummy columns and add back to the dataframe\n",
    "transform_df = pd.concat([\n",
    "    transform_df,\n",
    "    pd.get_dummies(transform_df.select_dtypes(include=['category']))\n",
    "], axis=1).drop(text_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29316.267922230298"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Update the select_features function with the findings above\n",
    "## Update the train_and_test function with a k value argument\n",
    "\n",
    "## Function for transforming the features based on criteria detailed below\n",
    "def transform_features(df):\n",
    "    ## Sum of null values for each column\n",
    "    null_values = df.isnull().sum()\n",
    "    \n",
    "    ## Drop columns with less than 5% missing values\n",
    "    over_5 = null_values[(null_values > len(df) * 0.05)].sort_values()\n",
    "    df = df.drop(over_5.index, axis=1)\n",
    "    \n",
    "    ## Replace numeric columns with missing values with the mode for that column\n",
    "    numeric_missing = df.select_dtypes(include=['float', 'int']).isnull().sum()\n",
    "    fix_columns = numeric_missing[numeric_missing > 0].sort_values()\n",
    "    \n",
    "    ## Create a dictionary with the mode for each fixable numeric column\n",
    "    replace_dict = df[fix_columns.index].mode().to_dict(orient='records')[0]\n",
    "    \n",
    "    ## Use the dictionary to fill in the values where missing\n",
    "    df = df.fillna(replace_dict)\n",
    "    \n",
    "    ## Find text columns with null values\n",
    "    text_missing = df.select_dtypes(include='object').isnull().sum()\n",
    "    \n",
    "    ## Remove columns with null values\n",
    "    drop_text = text_missing[text_missing > 0]\n",
    "    df = df.drop(drop_text.index, axis=1)\n",
    "    \n",
    "    ## Create a column from Year Sold and Year Built\n",
    "    df['Years Before Sale'] = df['Yr Sold'] - df['Year Built']\n",
    "    \n",
    "    ## Create a column from Year Sold and Year Remodelled \n",
    "    df['Years Since Remod'] = df['Yr Sold'] - df['Year Remod/Add']\n",
    "    \n",
    "    ## Drop rows with negative values for the new columns\n",
    "    df = df.drop([1702, 2180, 2181], axis=0)\n",
    "    \n",
    "    ## Drop columns no longer necessary columns\n",
    "    df = df.drop(['Year Built', 'Year Remod/Add'], axis=1)\n",
    "    \n",
    "    ## Drop columns that are not useful for ML\n",
    "    df = df.drop(['PID', 'Order'], axis=1)\n",
    "    \n",
    "    ## Drop columns that leak data about the sale\n",
    "    df = df.drop(['Mo Sold', 'Sale Condition', 'Sale Type', 'Yr Sold'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "## Function for selecting features for predicting based on criteria detailed below\n",
    "def select_features(df, corr_threshold=0.4, uniq_threshold=10):\n",
    "    ## Include only numeric columns\n",
    "    numeric_df = df.select_dtypes(include=['float', 'int'])\n",
    "    \n",
    "    ## List columns with their correlation to SalePrice\n",
    "    abs_corr = numeric_df.corr()['SalePrice'].abs().sort_values()\n",
    "    \n",
    "    ##Apply cor_cutoff to drop rows\n",
    "    df = df.drop(abs_corr[abs_corr < corr_threshold].index, axis=1)\n",
    "    \n",
    "    ## List of categorical columns from the documentation\n",
    "    cat_columns = [\"PID\", \n",
    "               \"MS SubClass\", \n",
    "               \"MS Zoning\", \n",
    "               \"Street\", \n",
    "               \"Alley\", \n",
    "               \"Land Contour\", \n",
    "               \"Lot Config\", \n",
    "               \"Neighborhood\", \n",
    "               \"Condition 1\", \n",
    "               \"Condition 2\", \n",
    "               \"Bldg Type\", \n",
    "               \"House Style\", \n",
    "               \"Roof Style\", \n",
    "               \"Roof Matl\", \n",
    "               \"Exterior 1st\", \n",
    "               \"Exterior 2nd\", \n",
    "               \"Mas Vnr Type\", \n",
    "               \"Foundation\", \n",
    "               \"Heating\", \n",
    "               \"Central Air\", \n",
    "               \"Garage Type\", \n",
    "               \"Misc Feature\", \n",
    "               \"Sale Type\", \n",
    "               \"Sale Condition\"\n",
    "              ]\n",
    "    \n",
    "    ## Limit the list to only those columns in our dataframe\n",
    "    current_cat_columns = []\n",
    "    for col in cat_columns:\n",
    "        if col in df.columns:\n",
    "            current_cat_columns.append(col)\n",
    "        \n",
    "    ## Find the unique values in each column\n",
    "    uniq_counts = df[current_cat_columns].apply(lambda col: len(col.value_counts())).sort_values()\n",
    "    \n",
    "    ## Apply the cutoff to the dataframe\n",
    "    drop_uniq_cols = uniq_counts[uniq_counts > uniq_threshold].index\n",
    "    df = df.drop(drop_uniq_cols, axis=1)\n",
    "    \n",
    "    ## Create a list of text columns\n",
    "    text_cols = df.select_dtypes(include=['object'])\n",
    "    \n",
    "    ## Convert text columns to object columns\n",
    "    for col in text_cols:\n",
    "        df[col] = df[col].astype('category')\n",
    "    \n",
    "    ## Create dummy columns and add back to the dataframe\n",
    "    df = pd.concat([\n",
    "        df,\n",
    "        pd.get_dummies(df.select_dtypes(include=['category']))\n",
    "    ], axis=1).drop(text_cols, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "## Function for splitting a dataframe into train and test and returning the RMSE of the predictions\n",
    "## Added k value argument\n",
    "\n",
    "def train_and_test(df, k=0):\n",
    "    ## Filter out only the numeric columns\n",
    "    n_df = df.select_dtypes(include=['integer', 'float'])\n",
    "    \n",
    "    ## Create a list of all columns without 'SalePrice'\n",
    "    features = n_df.columns.drop('SalePrice')\n",
    "    \n",
    "    ## Use Linear Regression\n",
    "    lr = linear_model.LinearRegression()\n",
    "    \n",
    "    ## For k=0 (default)\n",
    "    if k == 0:\n",
    "        train = df[:1460]\n",
    "        test = df[1460:]\n",
    "        \n",
    "        lr.fit(train[features], train['SalePrice'])\n",
    "        predictions = lr.predict(test[features])\n",
    "        mse = mean_squared_error(test['SalePrice'], predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        return rmse\n",
    "    \n",
    "    ## For k=1\n",
    "    if k == 1:\n",
    "        ## Shuffle the rows\n",
    "        shuffled = df.sample(frac=1)\n",
    "        fold_one = shuffled[:1460]\n",
    "        fold_two = shuffled[1460:]\n",
    "        \n",
    "        ## Train on Fold One and Test on Fold Two\n",
    "        lr.fit(fold_one[features], fold_one['SalePrice'])\n",
    "        prediction_one = lr.predict(fold_two[features])\n",
    "        mse_one = mean_squared_error(fold_two['SalePrice'], prediction_one)\n",
    "        rmse_one = np.sqrt(mse_one)\n",
    "        \n",
    "        ## Train on Fold Two and Test on Fold One\n",
    "        lr.fit(fold_two[features], fold_two['SalePrice'])\n",
    "        prediction_two = lr.predict(fold_one[features])\n",
    "        mse_two = mean_squared_error(fold_one['SalePrice'], prediction_two)\n",
    "        rmse_two = np.sqrt(mse_two)\n",
    "        \n",
    "        ## Find the mean of the two rmse\n",
    "        avg_rmse = np.mean([rmse_one, rmse_two])\n",
    "        \n",
    "        return avg_rmse\n",
    "    \n",
    "    else:\n",
    "        kf = KFold(n_splits=k, shuffle=True)\n",
    "        rmse_values = []\n",
    "        \n",
    "        for train_index, test_index, in kf.split(df):\n",
    "            train = df.iloc[train_index]\n",
    "            test = df.iloc[test_index]\n",
    "            lr.fit(train[features], train[\"SalePrice\"])\n",
    "            predictions = lr.predict(test[features])\n",
    "            mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "            rmse = np.sqrt(mse)\n",
    "            rmse_values.append(rmse)\n",
    "        \n",
    "        avg_rmse = np.mean(rmse_values)\n",
    "        \n",
    "        return avg_rmse\n",
    "\n",
    "## Test the functions\n",
    "df_final = pd.read_csv('AmesHousing.tsv', delimiter='\\t')\n",
    "transform_df_final = transform_features(df_final)\n",
    "filtered_df_final = select_features(transform_df_final)\n",
    "rmse_final = train_and_test(filtered_df_final, k=4)\n",
    "\n",
    "rmse_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
